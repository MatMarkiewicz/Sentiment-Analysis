{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this project is to predict the sentiments of sentences. <br>\n",
    "We work with IMDB movies reviews dataset. Movies are rated from 1 to 10. We consider only the reviews with rating 1 (the worst ones) and 10 (the best ones).  <br>\n",
    "The assumption is that movies with rating 1 have negative sentiment and movies with rating 10 have a positive sentiment.  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train dataset consists of 4732 reviews with rating 10 and 5100 reviews with rating 1. <br>\n",
    "A simple classifier that always votes for one class gives us a baseline accuracy of 51.87 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from obvious procedures like removing punctuation and lowering all sentences, we try different preprocessing techniques such as: <br>\n",
    "\n",
    "1. Stemming, (reducing words to their root form, we decrease the size of words dictionary).\n",
    "2. Stop words removal, (we assume that words like 'the', 'and', 'a/an' etc. don't give much information).\n",
    "3. N-grams, (considering contiguous sequences of n items from a given sample of text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemented classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CountVectorizer \n",
    "2. Naive Bayes\n",
    "3. Logistic Regression\n",
    "4. SVM\n",
    "5. Heuristic Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive Bayes classifier uses the Bayes theorem to classify data.  \n",
    "\n",
    "Bayes theorem:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) * P(A)}{P(B)}$$\n",
    "\n",
    "Given sentace \"this is a good movie\" we can say, that:\n",
    "\n",
    "$$ P(c=1 | \\text{this is a good movie}) = \\frac{P(\\text{this is a good movie}|c=1) * P(c=1)}{P(\\text{this is a good movie})}  $$\n",
    "where $ P(\\text{this is a good movie}) $ is a constant equal for both classes. \n",
    "\n",
    "Going further we can assume that\n",
    "\n",
    "$$ P(\\text{this is a good movie}|c=1) * P(c=1) \\approx P(\\text{this}|c=1) * P(\\text{is}|c=1) * P(\\text{a}|c=1) * P(\\text{good}|c=1) * P(\\text{movie}|c=1) * P(c=1) $$\n",
    "\n",
    "And we can use our train data to compute $P(c | \\text{word}) $ for all classes and words appears in train sentances and P(c) which can be equal fraction of samples with class $c$ in train data or chosen class prior for class $c$.\n",
    "\n",
    "During predictions of test data classes we can had to use $P(c | \\text{word}) $ for word that does not appear in train data. Then we can say that this probability is equal to choosen parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we tune the alpha parameter. <br>\n",
    "![alt text](Images/naive_bayes1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Naive Bayes accuracy. <br>\n",
    "![alt text](Images/naive_bayes2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklean Multinomial  Naive Bayes accuracy.\n",
    "![alt text](Images/naive_bayes3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our implementation and sklearn's.\n",
    "![alt text](Images/naive_bayes4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO (summary bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of logistic regresion is to calculate probability of the sample $x$ belonging to class 1.\n",
    "$$p(y=1|x) = \\sigma(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}$$  \n",
    "\n",
    "We can observe that:  \n",
    "$$ p(y=y^{(i)}|x^{(i)};\\Theta) = \\sigma(\\Theta^Tx)^{y^{(i)}}(1-\\sigma(\\Theta^Tx))^{(1-y^{(i)})}$$  \n",
    "\n",
    "Therefore the negative log likelihood ($nll$) is:$$\n",
    "\\begin{split}\n",
    "nll(\\Theta) &= -\\sum_{i=1}^{N} y^{(i)} \\log \\sigma(\\Theta^Tx) + (1-y^{(i)})\\log(1-\\sigma(\\Theta^Tx)) = \\\\\n",
    "&= -\\sum_{i=1}^{N}y^{(i)}\\log p(y=1|x^{(i)}; \\Theta) + (1-y^{(i)})\\log  p(y=0|x^{(i)}; \\Theta)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "So we are searching for $\\theta$:\n",
    "$$\\theta = arg\\,min_{\\theta} \\ nll(\\theta) $$  \n",
    "  \n",
    "We can further consider logistic regression with regularization, where:$$\n",
    "\\begin{split}\n",
    "nll(\\Theta) &= -\\sum_{i=1}^{N}y^{(i)}\\log p(y=1|x^{(i)}; \\Theta) + (1-y^{(i)})\\log  p(y=0|x^{(i)}; \\Theta) + \\frac{\\lambda}{2} \\sum_{i}\\theta_{i}^{2}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "There are a few ways to find $\\theta$. We will consider L-BFGS-B solver, then we will compare results with sklearn LogisticRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Logistic Regression accuracy. <br>\n",
    "![alt text](Images/lr1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn Logistic Regression accuracy. <br>\n",
    "![alt text](Images/lr2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our implementation and sklearn's.\n",
    "![alt text](Images/lr3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO (summary LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO (introducing to ngram and tf-idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between:\n",
    "1. Sklearn Logistic Regression + CV\n",
    "2. Sklearn Logistic Regression + CV + n-gram (1, 5)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "![alt text](Images/lr4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between:\n",
    "1. Sklearn Logistic Regression + CV\n",
    "2. Sklearn Logistic Regression + CV + n-gram (1, 5)\n",
    "3. Sklearn Logistic Regression + TF-IDF + n-gram (1, 5)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "![alt text](Images/lr5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO (summay of ngram and tf-idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the SVM is to predict class of $x^{(i)}$ using $\\text{signum}(w^T x + b)$\n",
    "\n",
    "SVM try to find weights $w\\in\\mathbb{R}^n$ and bias $b\\in\\mathbb{R}$ that maximize the separation margin. This corresponds to solving the following quadratic optimization problem:\n",
    "\n",
    "$$\\begin{split}\n",
    "  \\min_{w,b,\\xi}  &\\frac{1}{2}w^Tw  + C\\sum_{i=1}^m \\xi_i  \\\\\n",
    "  \\text{s.t. } & y^{(i)}(w^T x^{(i)} + b) \\geq 1- \\xi_i\\;\\; \\forall_i \\\\\n",
    "  & \\xi_i \\geq 0 \\;\\; \\forall_i.\n",
    "\\end{split}$$\n",
    "\n",
    "We used dual form of this problem with kernel trick method, this corresponds to solving the following problem:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\min_{\\alpha}  \\frac{1}{2}  \\alpha^T \\mathbf{H}  \\alpha - 1^T \\alpha\n",
    "    \\\\\n",
    "     s.t.  - \\alpha_i \\leq 0 \n",
    "    \\\\\n",
    "      \\alpha_i \\leq C\n",
    "     \\\\\n",
    "     \\ y^T \\alpha = 0  \n",
    "     \\\\ \n",
    "     \\ H = y^TKy  \n",
    "     \\\\\n",
    "     \\ K_{i,j} = K(x_i,x_j)\n",
    "\\end{aligned}$$\n",
    "  \n",
    "We used qp solver to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our SVM accuracy.\n",
    "\n",
    "![alt text](Images/svm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn SVM accuracy.\n",
    "![alt text](Images/svm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our implementation and sklearn's on train.\n",
    "![alt text](Images/svm3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our implementation and sklearn's on test.\n",
    "![alt text](Images/svm4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO (SVM summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We came up with some heuristic approaches and implemented them in Naive Bayes. <br>\n",
    "\n",
    "1. If we come across a negation word, we negate ${k}$ words after it. I.e. we change the next ${k}$ words' probability for probability from another class.\n",
    "2. For words that enhance sentiment we multiply the next ${k}$ words' probability.\n",
    "3. There are also lists with positive and negative words, because sometimes we have a positive word from the negative class and vice versa. We swap probability in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracies we got. <br>\n",
    "\n",
    "![alt text](Images/heura1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our Heuristic Naive Bayes and Naive Bayes.\n",
    "![alt text](Images/heura2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO  (Heuristic model summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary (to rewrite?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(All of below accuracies are calculated on test data.) <br>\n",
    "First of all the baseline accuracy was around 52%. Our Naive Bayes classifier scored 89% similarly to sklearn's implementation. <br>\n",
    "Using Logistic Regression we scored nearly 89.9% and sklearn got 92.4%. What is interesting our model worked better on stemmed data without stopwords contrary to sklearn's. <br>\n",
    "Further, we tested sklearn's countvectorizer with n-grams. For ngram_range=(1,4) and every other, where upper limit was greater than 4 we obtained 92.86% accuracy for original data and 92.77% accuracy for stemmed data without stop words.\n",
    "TF-IDF vectorizer wasn't better than countvectorizer. <br>\n",
    "Our SVM scored 92%, which is a bit better than Logistic Regression. <br>\n",
    "Heuristic Naive Bayes didn't fail us and yielded solid 90% accuracy. <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
