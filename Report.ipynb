{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this project is to predict the sentiments of sentences. <br>\n",
    "We work with IMDB movies reviews dataset. Movies are rated from 1 to 10. We consider only the reviews with rating 1 (the worst ones) and 10 (the best ones).  <br>\n",
    "The assumption is that movies with rating 1 have negative sentiment and movies with rating 10 have a positive sentiment.  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train dataset consists of 4732 reviews with rating 10 and 5100 reviews with rating 1. <br>\n",
    "A simple classifier that always votes for one class gives us a baseline accuracy of 51.87 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from obvious procedures like removing punctuation and lowering all sentences, we try different preprocessing techniques such as: <br>\n",
    "\n",
    "1. Stemming, (reducing words to their root form, we decrease the size of words dictionary).\n",
    "2. Stop words removal, (we assume that words like 'the', 'and', 'a/an' etc. don't give much information).\n",
    "3. N-grams, (considering contiguous sequences of n items from a given sample of text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemented classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CountVectorizer \n",
    "2. Naive Bayes\n",
    "3. Logistic Regression\n",
    "4. SVM\n",
    "5. Heuristic Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we tune the alpha parameter. <br>\n",
    "![alt text](Images/naive_bayes1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Naive Bayes accuracy. <br>\n",
    "![alt text](Images/naive_bayes2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklean Multinomial  Naive Bayes accuracy.\n",
    "![alt text](Images/naive_bayes3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our implementation and sklearn's.\n",
    "![alt text](Images/naive_bayes4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Logistic Regression accuracy. <br>\n",
    "![alt text](Images/lr1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn Logistic Regression accuracy. <br>\n",
    "![alt text](Images/lr2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our implementation and sklearn's.\n",
    "![alt text](Images/lr3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between:\n",
    "1. Sklearn Logistic Regression + CV\n",
    "2. Sklearn Logistic Regression + CV + n-gram (1, 5)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "![alt text](Images/lr4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between:\n",
    "1. Sklearn Logistic Regression + CV\n",
    "2. Sklearn Logistic Regression + CV + n-gram (1, 5)\n",
    "3. Sklearn Logistic Regression + TF-IDF + n-gram (1, 5)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "![alt text](Images/lr5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our SVM accuracy.\n",
    "\n",
    "![alt text](Images/svm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn SVM accuracy.\n",
    "![alt text](Images/svm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our implementation and sklearn's on train.\n",
    "![alt text](Images/svm3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our implementation and sklearn's on test.\n",
    "![alt text](Images/svm4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We came up with some heuristic approaches and implemented them in Naive Bayes. <br>\n",
    "\n",
    "1. If we come across a negation word, we negate ${k}$ words after it. I.e. we change the next ${k}$ words' probability for probability from another class.\n",
    "2. For words that enhance sentiment we multiply the next ${k}$ words' probability.\n",
    "3. There are also lists with positive and negative words, because sometimes we have a positive word from the negative class and vice versa. We swap probability in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracies we got. <br>\n",
    "\n",
    "![alt text](Images/heura1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our Heuristic Naive Bayes and Naive Bayes.\n",
    "![alt text](Images/heura2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(All of below accuracies are calculated on test data.) <br>\n",
    "First of all the baseline accuracy was around 52%. Our Naive Bayes classifier scored 89% similarly to sklearn's implementation. <br>\n",
    "Using Logistic Regression we scored nearly 89.9% and sklearn got 92.4%. What is interesting our model worked better on stemmed data without stopwords contrary to sklearn's. <br>\n",
    "Further, we tested sklearn's countvectorizer with n-grams. For ngram_range=(1,4) and every other, where upper limit was greater than 4 we obtained 92.86% accuracy for original data and 92.77% accuracy for stemmed data without stop words.\n",
    "TF-IDF vectorizer wasn't better than countvectorizer. <br>\n",
    "Our SVM scored 92%, which is a bit better than Logistic Regression. <br>\n",
    "Heuristic Naive Bayes didn't fail us and yielded solid 90% accuracy. <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
